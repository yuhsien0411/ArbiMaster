import * as tf from '@tensorflow/tfjs-node';
import { EnhancedFundingRatePredictor } from '../predictors/EnhancedFundingRatePredictor.js';
import { AdvancedDataCollector } from '../utils/AdvancedDataCollector.js';

export class AdvancedTrainingManager {
  constructor() {
    this.predictor = new EnhancedFundingRatePredictor();
    this.dataCollector = new AdvancedDataCollector();
    this.trainingHistory = [];
    this.modelVersions = new Map();
  }

  // æ™ºèƒ½è¨“ç·´ç­–ç•¥
  async intelligentTraining(config = {}) {
    console.log('ğŸ§  é–‹å§‹æ™ºèƒ½è¨“ç·´...');
    
    const {
      symbols = ['BTC', 'ETH', 'BNB', 'XRP', 'SOL'],
      days = 90,
      validationSplit = 0.2,
      maxEpochs = 200,
      patience = 30,
      minImprovement = 0.001
    } = config;

    try {
      // 1. æ”¶é›†é«˜å“è³ªæ•¸æ“š
      console.log('ğŸ“¥ æ”¶é›†è¨“ç·´æ•¸æ“š...');
      const dataset = await this.dataCollector.generateComprehensiveDataset(symbols, days);
      
      if (dataset.data.length < 1000) {
        throw new Error('è¨“ç·´æ•¸æ“šä¸è¶³ï¼Œè‡³å°‘éœ€è¦1000å€‹æ¨£æœ¬');
      }

      // 2. æ•¸æ“šå“è³ªæª¢æŸ¥
      console.log('ğŸ” æª¢æŸ¥æ•¸æ“šå“è³ª...');
      const quality = dataset.quality;
      console.log(`æ•¸æ“šå“è³ª: ${quality.overall.toFixed(1)}%`);
      
      if (quality.overall < 70) {
        console.warn('âš ï¸ æ•¸æ“šå“è³ªè¼ƒä½ï¼Œå¯èƒ½å½±éŸ¿æ¨¡å‹æ€§èƒ½');
      }

      // 3. ç‰¹å¾µå·¥ç¨‹
      console.log('ğŸ”§ é€²è¡Œç‰¹å¾µå·¥ç¨‹...');
      const features = await this.predictor.extractAdvancedFeatures(dataset.data);
      
      if (features.length === 0) {
        throw new Error('ç‰¹å¾µæå–å¤±æ•—');
      }

      // 4. æ•¸æ“šåˆ†å‰²
      const { trainFeatures, trainLabels, valFeatures, valLabels } = 
        this.splitData(features, dataset.data, validationSplit);

      // 5. å»ºç«‹é›†æˆæ¨¡å‹
      console.log('ğŸ—ï¸ å»ºç«‹é›†æˆæ¨¡å‹...');
      const inputShape = [trainFeatures.shape[1]];
      const ensembleModels = await this.predictor.buildEnsembleModel(inputShape);
      
      // 6. è¨“ç·´å„å€‹æ¨¡å‹
      const trainingResults = [];
      
      for (let i = 0; i < ensembleModels.length; i++) {
        const modelName = ['LSTM', 'Transformer', 'DeepMLP'][i];
        console.log(`ğŸ¯ è¨“ç·´ ${modelName} æ¨¡å‹...`);
        
        const result = await this.trainModel(
          ensembleModels[i],
          trainFeatures,
          trainLabels,
          valFeatures,
          valLabels,
          {
            maxEpochs,
            patience,
            minImprovement,
            modelName
          }
        );
        
        trainingResults.push(result);
      }

      // 7. æ¨¡å‹é›†æˆ
      console.log('ğŸ”— é›†æˆæ¨¡å‹...');
      this.predictor.ensembleModels = ensembleModels;
      
      // 8. é©—è­‰é›†æˆæ¨¡å‹
      const ensemblePerformance = await this.validateEnsembleModel(
        valFeatures,
        valLabels
      );

      // 9. ä¿å­˜æ¨¡å‹
      await this.saveModels(ensembleModels, trainingResults);

      // 10. è¨˜éŒ„è¨“ç·´æ­·å²
      const trainingRecord = {
        timestamp: new Date().toISOString(),
        symbols,
        days,
        dataQuality: quality,
        trainingResults,
        ensemblePerformance,
        modelVersion: this.generateModelVersion()
      };
      
      this.trainingHistory.push(trainingRecord);
      this.saveTrainingHistory();

      console.log('âœ… æ™ºèƒ½è¨“ç·´å®Œæˆ');
      
      return {
        success: true,
        trainingRecord,
        ensemblePerformance
      };

    } catch (error) {
      console.error('âŒ æ™ºèƒ½è¨“ç·´å¤±æ•—:', error);
      return {
        success: false,
        error: error.message
      };
    }
  }

  // æ•¸æ“šåˆ†å‰²
  splitData(features, data, validationSplit) {
    const totalSamples = features.length;
    const valSize = Math.floor(totalSamples * validationSplit);
    const trainSize = totalSamples - valSize;

    // éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
    const indices = tf.util.createShuffledIndices(totalSamples);
    
    const trainIndices = indices.slice(0, trainSize);
    const valIndices = indices.slice(trainSize);

    // åˆ†å‰²ç‰¹å¾µ
    const trainFeatures = [];
    const valFeatures = [];
    
    for (let i = 0; i < trainSize; i++) {
      trainFeatures.push(features[trainIndices[i]]);
    }
    
    for (let i = 0; i < valSize; i++) {
      valFeatures.push(features[valIndices[i]]);
    }

    // åˆ†å‰²æ¨™ç±¤
    const trainLabels = [];
    const valLabels = [];
    
    for (let i = 0; i < trainSize; i++) {
      trainLabels.push(data[trainIndices[i]].fundingRate);
    }
    
    for (let i = 0; i < valSize; i++) {
      valLabels.push(data[valIndices[i]].fundingRate);
    }

    return {
      trainFeatures: tf.tensor2d(trainFeatures),
      trainLabels: tf.tensor2d(trainLabels, [trainLabels.length, 1]),
      valFeatures: tf.tensor2d(valFeatures),
      valLabels: tf.tensor2d(valLabels, [valLabels.length, 1])
    };
  }

  // è¨“ç·´å–®å€‹æ¨¡å‹
  async trainModel(model, trainFeatures, trainLabels, valFeatures, valLabels, config) {
    const { maxEpochs, patience, minImprovement, modelName } = config;
    
    let bestLoss = Infinity;
    let patienceCount = 0;
    let learningRate = 0.001;
    
    const history = {
      epochs: [],
      trainLoss: [],
      valLoss: [],
      trainMae: [],
      valMae: []
    };

    for (let epoch = 0; epoch < maxEpochs; epoch++) {
      // å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡
      if (epoch > 0 && epoch % 50 === 0) {
        learningRate *= 0.5;
        const optimizer = tf.train.adamax(learningRate);
        model.compile({
          optimizer,
          loss: 'huberLoss',
          metrics: ['mae']
        });
      }

      // è¨“ç·´ä¸€å€‹epoch
      const result = await model.fit(trainFeatures, trainLabels, {
        epochs: 1,
        batchSize: 32,
        validationData: [valFeatures, valLabels],
        shuffle: true,
        verbose: 0
      });

      const trainLoss = result.history.loss[0];
      const valLoss = result.history.val_loss[0];
      const trainMae = result.history.mae[0];
      const valMae = result.history.val_mae[0];

      // è¨˜éŒ„æ­·å²
      history.epochs.push(epoch + 1);
      history.trainLoss.push(trainLoss);
      history.valLoss.push(valLoss);
      history.trainMae.push(trainMae);
      history.valMae.push(valMae);

      // æ—©åœæª¢æŸ¥
      if (valLoss < bestLoss - minImprovement) {
        bestLoss = valLoss;
        patienceCount = 0;
      } else {
        patienceCount++;
      }

      // è¼¸å‡ºé€²åº¦
      if (epoch % 10 === 0) {
        console.log(`  Epoch ${epoch + 1}/${maxEpochs}: train_loss=${trainLoss.toFixed(6)}, val_loss=${valLoss.toFixed(6)}`);
      }

      // æ—©åœ
      if (patienceCount >= patience) {
        console.log(`  ğŸ›‘ ${modelName} æ—©åœæ–¼ç¬¬ ${epoch + 1} è¼ª`);
        break;
      }
    }

    return {
      modelName,
      bestLoss,
      finalEpoch: history.epochs.length,
      history,
      learningRate
    };
  }

  // é©—è­‰é›†æˆæ¨¡å‹
  async validateEnsembleModel(valFeatures, valLabels) {
    const predictions = [];
    const actuals = valLabels.arraySync().flat();

    // ç²å–æ¯å€‹æ¨¡å‹çš„é æ¸¬
    for (const model of this.predictor.ensembleModels) {
      const modelPredictions = model.predict(valFeatures);
      predictions.push(modelPredictions.arraySync().flat());
      modelPredictions.dispose();
    }

    // è¨ˆç®—é›†æˆé æ¸¬
    const weights = [0.4, 0.35, 0.25]; // LSTM, Transformer, MLP
    const ensemblePredictions = [];
    
    for (let i = 0; i < actuals.length; i++) {
      let weightedPred = 0;
      for (let j = 0; j < predictions.length; j++) {
        weightedPred += predictions[j][i] * weights[j];
      }
      ensemblePredictions.push(weightedPred);
    }

    // è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
    const mse = this.calculateMSE(ensemblePredictions, actuals);
    const mae = this.calculateMAE(ensemblePredictions, actuals);
    const rmse = Math.sqrt(mse);
    const r2 = this.calculateR2(ensemblePredictions, actuals);

    return {
      mse,
      mae,
      rmse,
      r2,
      predictions: ensemblePredictions,
      actuals
    };
  }

  // æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
  calculateMSE(predictions, actuals) {
    let sum = 0;
    for (let i = 0; i < predictions.length; i++) {
      sum += Math.pow(predictions[i] - actuals[i], 2);
    }
    return sum / predictions.length;
  }

  calculateMAE(predictions, actuals) {
    let sum = 0;
    for (let i = 0; i < predictions.length; i++) {
      sum += Math.abs(predictions[i] - actuals[i]);
    }
    return sum / predictions.length;
  }

  calculateR2(predictions, actuals) {
    const mean = actuals.reduce((sum, val) => sum + val, 0) / actuals.length;
    
    let ssRes = 0;
    let ssTot = 0;
    
    for (let i = 0; i < predictions.length; i++) {
      ssRes += Math.pow(predictions[i] - actuals[i], 2);
      ssTot += Math.pow(actuals[i] - mean, 2);
    }
    
    return 1 - (ssRes / ssTot);
  }

  // ä¿å­˜æ¨¡å‹
  async saveModels(models, trainingResults) {
    console.log('ğŸ’¾ ä¿å­˜æ¨¡å‹...');
    
    const modelVersion = this.generateModelVersion();
    const basePath = `./models/ensemble_${modelVersion}`;
    
    for (let i = 0; i < models.length; i++) {
      const modelName = ['lstm', 'transformer', 'mlp'][i];
      const modelPath = `${basePath}/${modelName}`;
      
      await models[i].save(`file://${modelPath}`);
      
      // ä¿å­˜è¨“ç·´çµæœ
      const resultPath = `${modelPath}/training_result.json`;
      const fs = require('fs');
      fs.writeFileSync(resultPath, JSON.stringify(trainingResults[i], null, 2));
    }

    // ä¿å­˜é›†æˆé…ç½®
    const ensembleConfig = {
      version: modelVersion,
      timestamp: new Date().toISOString(),
      weights: [0.4, 0.35, 0.25],
      modelNames: ['lstm', 'transformer', 'mlp'],
      basePath
    };
    
    const configPath = `${basePath}/ensemble_config.json`;
    const fs = require('fs');
    fs.writeFileSync(configPath, JSON.stringify(ensembleConfig, null, 2));

    this.modelVersions.set(modelVersion, ensembleConfig);
    console.log(`âœ… æ¨¡å‹å·²ä¿å­˜åˆ° ${basePath}`);
  }

  // ç”Ÿæˆæ¨¡å‹ç‰ˆæœ¬
  generateModelVersion() {
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    return `v${timestamp}`;
  }

  // ä¿å­˜è¨“ç·´æ­·å²
  saveTrainingHistory() {
    const fs = require('fs');
    const historyPath = './models/training_history.json';
    fs.writeFileSync(historyPath, JSON.stringify(this.trainingHistory, null, 2));
  }

  // è¼‰å…¥æœ€ä½³æ¨¡å‹
  async loadBestModel() {
    console.log('ğŸ“‚ è¼‰å…¥æœ€ä½³æ¨¡å‹...');
    
    // æ ¹æ“šé©—è­‰æå¤±é¸æ“‡æœ€ä½³æ¨¡å‹
    if (this.trainingHistory.length === 0) {
      throw new Error('æ²’æœ‰å¯ç”¨çš„è¨“ç·´æ­·å²');
    }

    const bestRecord = this.trainingHistory.reduce((best, current) => {
      const currentLoss = current.ensemblePerformance.mse;
      const bestLoss = best.ensemblePerformance.mse;
      return currentLoss < bestLoss ? current : best;
    });

    const modelVersion = bestRecord.modelVersion;
    const config = this.modelVersions.get(modelVersion);
    
    if (!config) {
      throw new Error(`æ‰¾ä¸åˆ°æ¨¡å‹ç‰ˆæœ¬: ${modelVersion}`);
    }

    // è¼‰å…¥å„å€‹æ¨¡å‹
    const models = [];
    for (const modelName of config.modelNames) {
      const modelPath = `${config.basePath}/${modelName}`;
      const model = await tf.loadLayersModel(`file://${modelPath}/model.json`);
      models.push(model);
    }

    this.predictor.ensembleModels = models;
    console.log(`âœ… å·²è¼‰å…¥æœ€ä½³æ¨¡å‹ç‰ˆæœ¬: ${modelVersion}`);
    
    return {
      modelVersion,
      performance: bestRecord.ensemblePerformance
    };
  }

  // æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ
  compareModels() {
    if (this.trainingHistory.length < 2) {
      console.log('éœ€è¦è‡³å°‘2å€‹æ¨¡å‹ç‰ˆæœ¬é€²è¡Œæ¯”è¼ƒ');
      return null;
    }

    const comparison = this.trainingHistory.map(record => ({
      version: record.modelVersion,
      timestamp: record.timestamp,
      mse: record.ensemblePerformance.mse,
      mae: record.ensemblePerformance.mae,
      r2: record.ensemblePerformance.r2,
      dataQuality: record.dataQuality.overall
    }));

    // æŒ‰MSEæ’åº
    comparison.sort((a, b) => a.mse - b.mse);

    console.log('ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ:');
    comparison.forEach((model, index) => {
      console.log(`${index + 1}. ${model.version}: MSE=${model.mse.toFixed(6)}, RÂ²=${model.r2.toFixed(3)}`);
    });

    return comparison;
  }

  // è‡ªå‹•åŒ–è¨“ç·´æµç¨‹
  async autoTraining() {
    console.log('ğŸ¤– é–‹å§‹è‡ªå‹•åŒ–è¨“ç·´æµç¨‹...');
    
    const configs = [
      { symbols: ['BTC', 'ETH'], days: 30, maxEpochs: 100 },
      { symbols: ['BTC', 'ETH', 'BNB'], days: 60, maxEpochs: 150 },
      { symbols: ['BTC', 'ETH', 'BNB', 'XRP', 'SOL'], days: 90, maxEpochs: 200 }
    ];

    const results = [];
    
    for (const config of configs) {
      console.log(`\nğŸ”„ å˜—è©¦é…ç½®: ${JSON.stringify(config)}`);
      
      const result = await this.intelligentTraining(config);
      results.push({
        config,
        result
      });

      if (result.success) {
        console.log(`âœ… é…ç½®æˆåŠŸï¼Œæ€§èƒ½: MSE=${result.ensemblePerformance.mse.toFixed(6)}`);
      } else {
        console.log(`âŒ é…ç½®å¤±æ•—: ${result.error}`);
      }
    }

    // é¸æ“‡æœ€ä½³é…ç½®
    const successfulResults = results.filter(r => r.result.success);
    if (successfulResults.length > 0) {
      const bestResult = successfulResults.reduce((best, current) => {
        const currentMSE = current.result.ensemblePerformance.mse;
        const bestMSE = best.result.ensemblePerformance.mse;
        return currentMSE < bestMSE ? current : best;
      });

      console.log(`\nğŸ† æœ€ä½³é…ç½®: ${JSON.stringify(bestResult.config)}`);
      console.log(`æœ€ä½³æ€§èƒ½: MSE=${bestResult.result.ensemblePerformance.mse.toFixed(6)}`);
      
      return bestResult;
    } else {
      console.log('âŒ æ‰€æœ‰é…ç½®éƒ½å¤±æ•—äº†');
      return null;
    }
  }
} 